{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top_of_page\"></a>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Area of Control with Reinforcement Learning\n",
    "\n",
    "### *By David Skarbrevik, Booz Allen Hamilton*\n",
    "\n",
    "### <Skarbrevik_David@bah.com>\n",
    "\n",
    "---\n",
    "\n",
    "*Booz Allen Hamilton Inc. Proprietary and Confidential Information. The information provided herein contains Booz Allen Hamilton proprietary, confidential, and trade secret information and is provided solely for the client identified for informational and/or evaluation purposes only. The information contained in this document/briefing shall not be duplicated, used, or disclosed in whole or in part without the express written permission of an authorized officer of Booz Allen Hamilton.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to view this notebook online:\n",
    "\n",
    "<span style=\"font-size:28px;\">https://bit.ly/2PywzWy</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"background-color: black; padding: 1px;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"font-size:24px;\">Table of Contents</span>\n",
    "\n",
    "<br>\n",
    "\n",
    "<ol>\n",
    "    <span style=\"font-size:20px;\"><li><a href=\"#section1\">Reinforcement Learning Introduction</a></li></span>\n",
    "    <br>\n",
    "    <br>\n",
    "    <span style=\"font-size:20px;\"><li><a href=\"#section2\">Application to Navy and DoD Space</a></li></span>\n",
    "    <br>\n",
    "    <br>\n",
    "    <span style=\"font-size:20px;\"><li><a href=\"#section3\">Beyond this Tutorial</a></li></span>  \n",
    "</ol>\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"background-color: black; padding: 1px; margin-top:-5px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "    <a href=\"#top\">back to top</a>\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size:28px;\">Reinforcement Learning (RL) Introduction</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Reinforcement Learning?\n",
    "\n",
    "\"Reinforcement learning is learning what to do—how to map situations to actions—so\n",
    "as to maximize a numerical reward signal.\" - Richard S. Sutton and Andrew G. Barto (Reinforcement Learning 2nd ed.)\n",
    "\n",
    "If you've heard of bots that play chess or video games; or if you've seen robots learn to walk, then you've probably seen reinforcement learning at work. \n",
    "\n",
    "In this tutorial we're interested in a specific class of reinforcement learning called **temporal difference learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Reinforcement Learning?\n",
    "\n",
    "There are an endless variety of machine learning models being used to solve a wide array of problems, so what makes reinforcement learning special? What does it offer that some sophisticated neural network doesn't?\n",
    "\n",
    "Reinforcement learning problems have a time element to them. You take one action in an environment and then you take another. But, unlike the time series problems of predicting stock prices or generating a coherent sentence, reinforcement learning problems often assume a Markov Decision Process (MDP). This means that past states are not important to determining future states. For instance, this is true in chess because on any given move we have all the information we need to know how best to proceed forward. Still while a Recurrent Neural Network would typically be used in a language translation problem, you will still see reinforcement learning used for things like bitcoin or stock price prediction as this MDP assumption is not always so strict.\n",
    "\n",
    "Aside from often revolving around Markov Decision Processes, reinforcement learning is marked by the idea of trial and error. Many reinforcement learning algorithms are \"model free\" meaning they don't have any built in assumptions about how the world around them works. Reinforcement learning agents simply deduce the way a world works by interacting with it.\n",
    "\n",
    "\n",
    "\"These two characteristics — **trial-and-error search** and **delayed reward** — are the two most important\n",
    "distinguishing features of reinforcement learning.\" - Richard S. Sutton and Andrew G. Barto (Reinforcement Learning 2nd ed.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Terminology in Reinforcement Learning\n",
    "\n",
    "**Agent** - the \"learner\" and decision maker.\n",
    "\n",
    "**Environment** - everything the agent interacts with.\n",
    "\n",
    "**State** - the actual values of the environment at any specific point in time.\n",
    "\n",
    "**Actions** - the choices an agent has to perform in an environment.\n",
    "\n",
    "**Rewards** - the feedback an agent receives after performing actions in an environment.\n",
    "\n",
    "**Policy** - The strategy that an agent uses to determine its next action based on its current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./rl_diagram.gif\" style=\"height:220px; margin-left:60px;   display: block; margin-left: auto; margin-right: auto; width: 50%;\">\n",
    "\n",
    "<span style=\"float:right\">source: <a href=\"https://i.pinimg.com/originals/95/ac/06/95ac061420a4061579ec2029739a30da.gif\">easyai.tech</a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning and Deep Q Networks (a specific form of reinforcement learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q learning comes from a sepcific class of reinforcement learning algorithms known as temporal difference (TD) learning algorithms. Q learning has the ability\n",
    "\n",
    "Put simply, Q learning is about mapping all possible environment states and agent actions so that we know the optimal choice in any situation.\n",
    "\n",
    "Consider the example of a delivery business. If we wanted to optimize our delivery routes we might have a Q table like you see below. We have 500 possible states our agent can be in (could vary depending on the complexity of our delivery area) and 6 possible actions our agent can perform. Initially all values in our table are set to 0, so there is no obvious best action for the agent to take in any given state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./qlearning_table.png\">\n",
    "<span style=\"float:right\">source: <a href=\"https://en.wikipedia.org/wiki/Q-learning\">Wikpedia</a></span>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "After we have trained our agent by letting it explore the environment, it will learn Q values that help it make optimal choices. For instance if the agent enters state 499 we see that it is decisively the best choice for it to move west."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Q in Q learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what exactly does \"training\" mean for Q learning? How do we get useful values in our Q table?\n",
    "\n",
    "We can think of Q learning as the \"policy\" we use to train our agent. Below is the equation used to update the values in our Q table.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<img src=\"./qlearning.svg\">\n",
    "<span style=\"float:right\">source: <a href=\"https://en.wikipedia.org/wiki/Q-learning\">Wikpedia</a></span>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Without going into the math too much here, suffice to say that this equation let's us update our Q table values so that we can make smarter choices as we let our agent explore its environment longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Q Networks (DQN):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have an environment that has thousands of possible states and hundreds of possible actions. The Q table for this situation would be massive and learning optimal values for the entire table would take forever.\n",
    "\n",
    "Deep Q learning takes standard Q learning and adds a neural network to make the final decision about what action should be taken in any given state. The advantage is that with the added neural network we can efficiently learn useful policies for very complex environments. The inner workings of the neural networks used in DQN models is beyond the scope of this tutorial but look at resources in the \"Beyond this Tutorial\" section at the end of this notebook for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple and Concrete Example (Google DeepMind - Atari)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2013 DeepMind made use of DQNs to train a reinforcement learning agent to play Atari \"Breakout\". An important idea about \"model-free\" learning is that there are no assumptions about how the environment works. Our agent is free to try any strategy possible. In the case of this Atari \"Breakout\" game, the DQN agent taught DeepMind researchers that a highly efficient strategy is to tunnel a path up and destroy blocks from the top down.\n",
    "\n",
    "resource: <a href=\"https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning\">DeepMind Publication</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"deepmind.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"deepmind.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"float:right\">source: <a href=\"https://www.youtube.com/watch?v=V1eYniJ0Rnk\">Two Minute Papers</a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q learning doesn't require a model (model-free).\n",
    "\n",
    "* Q learning updates its predictions as it steps through the environment and doesn't wait for an entire game/session/episode to end before updating.\n",
    "\n",
    "* Reinforcement learning uses a time based feedback loop to learn the best action to take in a given state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./rl_diagram2.png\" style=\"height:220px; margin-left:60px;   display: block; margin-left: auto; margin-right: auto; width: 50%;\">\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"float:right\">source: <a href=\"https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287\">Towards Data Science</a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "    <a href=\"#top\">back to top</a>\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size:28px;\">Application to Navy and DoD space</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course reinforcement learning is useful for much more than just video games. In the example below we have two forces trying to find and capture each other's base and the surrounding land. We'll see how a DQN agent can be used to efficiently look for the enemy base.\n",
    "\n",
    "This will be a simplified scenario where a section of space is represented by a small grid and it is assumed that an agent can capture any space as long as its base is intact. It is also assumed that the agents can only move up, down, left, or right. A more realistic environment will likely have a larger state space and more possible actions, but this will serve as a useful prototype to showcase the usefulness of DQNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./navy_rl_start.png\" style=\"height:400px;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Red and blue squares represent the two forces looking to find and take over each other's base (represented by the light blue and orange squares). Initially the agents controlling the blue and red forces have no idea what they need to do or how to do it optimally, but with experience they learn their goal and how to optimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"navy_rl_untrained.mov\" controls  width=\"600\"  height=\"600\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"navy_rl_untrained.mov\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python packages we'll be using**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # for array stuff and random\n",
    "from PIL import Image  # for creating visual of our env\n",
    "import matplotlib.pyplot as plt  # for graphing our mean rewards over time\n",
    "import pickle  # to save/load Q-Tables\n",
    "from matplotlib import style  # to make pretty charts because it matters.\n",
    "import time  # using this to keep track of our saved Q-Tables.\n",
    "import itertools\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from pympler import tracker\n",
    "import cv2  # for showing our visual live\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "#import tensorflow.keras.backend as K\n",
    "from collections import deque\n",
    "import gc\n",
    "\n",
    "style.use(\"ggplot\")  # setting our style!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.99\n",
    "REPLAY_MEMORY_SIZE = 50_000  # How many last steps to keep for model training\n",
    "MIN_REPLAY_MEMORY_SIZE = 5_000  # Minimum number of steps in a memory to start training\n",
    "MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
    "MODEL_NAME = '2x128'\n",
    "MIN_REWARD = 0  # For model save\n",
    "MEMORY_FRACTION = 0.20\n",
    "\n",
    "SIZE = 10\n",
    "\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1  # not a constant, going to be decayed\n",
    "EPSILON_DECAY = 0.975\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "#  Stats settings\n",
    "RECORD_EVERY = 1  # episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DQN Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, saved_weights=None):\n",
    "        # Main model: this gets .fit every step\n",
    "        self.saved_weights = saved_weights\n",
    "\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "        # Target model: this is what we .predict against every step\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        if saved_weights:\n",
    "            self.target_model.load_weights(saved_weights)\n",
    "            \n",
    "        # used to .fit off of a batch, rather than just a single value\n",
    "        # This helps to smooth things out\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "        \n",
    "        #self.tensorboard = ModifiedTensorBoard(log_dir = f\"logs/{MODEL_NAME}-{int(time.time())}\")\n",
    "        self.target_update_counter = 0\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(128, (3, 3), input_shape=env.OBSERVATION_SPACE_VALUES))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "        model.add(Conv2D(128, (3, 3)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64))\n",
    "        model.add(Dense(env.ACTION_SPACE_SIZE, activation=\"sigmoid\"))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "        #model.compile(loss=\"mse\", optimizer=AdamOptimizer(), metrics=['accuracy'])\n",
    "        \n",
    "\n",
    "        return model\n",
    "    \n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "    \n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n",
    "    \n",
    "    def train(self, terminal_state, step):\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "        \n",
    "        current_states = np.array([transition[0] for transition in minibatch])/255\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "        \n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])/255\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for index, (current_states, action, reward, new_current_states, done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "                \n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "            \n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        self.model.fit(np.array(X)/255, np.array(y),\n",
    "                       batch_size=MINIBATCH_SIZE,\n",
    "                       verbose=0,\n",
    "                       shuffle=False)#,\n",
    "                       #callbacks=[self.tensorboard] if terminal_state else None)\n",
    "                \n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "        \n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "    <a href=\"#top\">back to top</a>\n",
    "</div>\n",
    "\n",
    "### Building our Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Blob:\n",
    "    def __init__(self, x, y):#, col):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.BASE = (x,y)\n",
    "        self.can_capture = True\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"({self.x}, {self.y})\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"({self.x}, {self.y})\"\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return (self.x-other.x, self.y-other.y)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "    \n",
    "    def action(self, choice, blocked):\n",
    "        '''\n",
    "        Gives us 9 total movement options.\n",
    "        '''\n",
    "        if choice == 0:\n",
    "            self.move(x=1, y=1, blocked=blocked) # down-right\n",
    "        elif choice == 1:\n",
    "            self.move(x=-1, y=-1, blocked=blocked) # up-left\n",
    "        elif choice == 2:\n",
    "            self.move(x=-1, y=1, blocked=blocked) # down-left\n",
    "        elif choice == 3:\n",
    "            self.move(x=1, y=-1, blocked=blocked) # up-right\n",
    "        elif choice == 4:\n",
    "            self.move(x=1, y=0, blocked=blocked) # right\n",
    "        elif choice == 5:\n",
    "            self.move(x=-1, y=0, blocked=blocked) # left\n",
    "        elif choice == 6:\n",
    "            self.move(x=0, y=1, blocked=blocked) # down\n",
    "        elif choice == 7:\n",
    "            self.move(x=0, y=-1, blocked=blocked) # up\n",
    "        elif choice == 8:\n",
    "            self.move(x=0, y=0, blocked=blocked) # none\n",
    "    \n",
    "    def move(self, x=False, y=False, blocked=None):\n",
    "        temp_x = self.x\n",
    "        temp_y = self.y\n",
    "        # If no value for x, move randomly\n",
    "        if not x:\n",
    "            self.x += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.x += x\n",
    "\n",
    "        # If no value for y, move randomly\n",
    "        if not y:\n",
    "            self.y += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.y += y\n",
    "\n",
    "        # If we are out of bounds, fix!\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        elif self.x > SIZE-1:\n",
    "            self.x = SIZE-1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        elif self.y > SIZE-1:\n",
    "            self.y = SIZE-1\n",
    "        if (self.x == blocked[0]) & (self.y == blocked[1]):\n",
    "            self.x = temp_x\n",
    "            self.y = temp_y\n",
    "            \n",
    "    def get_loc(self):\n",
    "        return (self.x, self.y)\n",
    "    \n",
    "    def distance(self, other_x, other_y):\n",
    "        return ((self.x-other_x)**2 + (self.y-other_y)**2)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlobEnv:\n",
    "    SIZE = SIZE\n",
    "    OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)\n",
    "    ACTION_SPACE_SIZE = 9\n",
    "    # the dict! (colors)\n",
    "    d = {'red_agent': (0, 0, 255),\n",
    "         'blue_agent': (255, 0, 0),\n",
    "         'red_space': (100, 100, 255),\n",
    "         'blue_space': (255, 100, 100),\n",
    "         'red_base': (100, 200, 255),\n",
    "         'blue_base': (255, 200, 100)}\n",
    "\n",
    "    def reset(self):\n",
    "        self.red = Blob(np.random.randint(0, SIZE), np.random.randint(0, SIZE))\n",
    "        self.blue = Blob(np.random.randint(0, SIZE), np.random.randint(0, SIZE))\n",
    "        while(self.red == self.blue):\n",
    "            self.blue = Blob(np.random.randint(0, SIZE), np.random.randint(0, SIZE))\n",
    "\n",
    "\n",
    "        self.episode_step = 0\n",
    "        self.vals = np.ones((self.SIZE, self.SIZE))\n",
    "        self.state = np.zeros((self.SIZE, self.SIZE))\n",
    "\n",
    "        loc_red = self.red.get_loc()\n",
    "        loc_blue = self.blue.get_loc()\n",
    "        self.vals[loc_red] = 5\n",
    "        self.vals[loc_blue] = 5\n",
    "        self.state[loc_red] = 1 * self.vals[loc_red]\n",
    "        self.state[loc_blue] = -1 * self.vals[loc_blue]\n",
    "        \n",
    "        self.last_state = np.array(self.state)\n",
    "        return np.array(self.get_image())\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.episode_step += 1\n",
    "        self.red.action(actions[0], self.blue.get_loc())\n",
    "        self.blue.action(actions[1], self.red.get_loc())\n",
    "        \n",
    "        loc_red = self.red.get_loc()\n",
    "        loc_blue = self.blue.get_loc()\n",
    "        \n",
    "        #reward_red = ((self.state[loc_red] * -1 + 1) * 2) - 1 #was the tile already yours?\n",
    "\n",
    "        if(loc_red == self.blue.BASE and self.red.can_capture):\n",
    "            self.blue.can_capture = False\n",
    "        if(loc_blue == self.red.BASE and self.blue.can_capture):\n",
    "            self.red.can_capture = False\n",
    "        if(self.red.can_capture):\n",
    "            reward_red = self.state[loc_red] * -1 + 1\n",
    "            self.state[loc_red] = 1 * self.vals[loc_red]\n",
    "            to_capture = np.where(self.state <= 0)\n",
    "            reward_red += -1 * min([self.red.distance(x,y) for x, y in zip(to_capture[0],to_capture[1])])\n",
    "        else:\n",
    "            reward_red = 0\n",
    "        if(self.blue.can_capture):\n",
    "            reward_blue = self.state[loc_blue] + 1\n",
    "            self.state[loc_blue] = -1 * self.vals[loc_blue]\n",
    "            to_capture = np.where(self.state >= 0)\n",
    "            reward_blue += -1 * max([self.blue.distance(x,y) for x, y in zip(to_capture[0],to_capture[1])])\n",
    "        else:\n",
    "            reward_blue = 0\n",
    "        #reward_red += -(np.sign(self.state -1) != 1).sum() # number of spaces left to capture\n",
    "        #reward_red += (np.sign(self.state) - 1).sum()\n",
    "        #reward_red = self.state.sum() - self.last_state.sum()\n",
    "        #reward_red = ((self.state) - (self.last_state)).sum()\n",
    "        #reward_blue = self.state.sum() * -1\n",
    "        rewards = [reward_red, reward_blue]\n",
    "        done = False\n",
    "        if self.episode_step >= EPISODE_LEN:# or self.red.get_loc() == (self.SIZE-1, self.SIZE-1) or self.blue.get_loc() == (0,0):\n",
    "            done = True\n",
    "\n",
    "        self.last_state = np.array(self.state)\n",
    "        return np.array(self.get_image()), rewards, done\n",
    "\n",
    "    def render(self):\n",
    "        img = self.get_image()\n",
    "        img = img.resize((300, 300), resample=Image.NEAREST)  # resizing so we can see our agent in all its glory.\n",
    "        cv2.imshow(\"image\", np.array(img))  # show it!\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    # FOR CNN #\n",
    "    def get_image(self):\n",
    "        env = np.zeros((self.SIZE, self.SIZE, 3), dtype=np.uint8)  # starts an rbg of our size\n",
    "        env[np.sign(self.state) == 1] = self.d['red_space']\n",
    "        env[np.sign(self.state) == -1] = self.d['blue_space']\n",
    "        if(self.red.can_capture):\n",
    "            env[self.red.BASE] = self.d['red_base']\n",
    "        if(self.blue.can_capture):\n",
    "            env[self.blue.BASE] = self.d['blue_base']\n",
    "\n",
    "        env[self.red.get_loc()] = self.d['red_agent']\n",
    "        env[self.blue.get_loc()] = self.d['blue_agent']\n",
    "\n",
    "        img = Image.fromarray(env, 'RGB')  # reading to rgb. Apparently. Even tho color definitions are bgr. ???\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "    <a href=\"#top\">back to top</a>\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size:28px;\">Training our Reinforcement Learning Agent</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings for our enviornment and training session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlobEnv()\n",
    "\n",
    "red_agent = DQNAgent(\"./models/2x128__-71.90_1582577169.h5\")\n",
    "blue_agent = DQNAgent()\n",
    "SWITCH_EVERY = 25\n",
    "red_win = 0\n",
    "blue_win = 0\n",
    "draw = 0\n",
    "\n",
    "# Environment settings\n",
    "EPISODES = 1\n",
    "EPISODE_LEN = 200\n",
    "SHOW_EVERY = 5\n",
    "\n",
    "countdown = 0\n",
    "best_ep_rewards = -200\n",
    "TERMINATE_COUNT = 50\n",
    "\n",
    "# For stats\n",
    "ep_rewards = []#[-10]\n",
    "\n",
    "# For more repetitive results\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performing our training steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3274eb9f1455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mSHOW_EVERY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Every step we update replay memory and train main network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate over episodes\n",
    "for episode in range(1, EPISODES + 1):\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "\n",
    "    # Reset environment and get initial state\n",
    "    current_state = env.reset()\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    while not done:\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            red_action = np.argmax(red_agent.get_qs(current_state))\n",
    "        else:\n",
    "            # Get random action\n",
    "            red_action = np.random.randint(0, env.ACTION_SPACE_SIZE)\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            blue_action = np.argmax(blue_agent.get_qs(current_state))\n",
    "        else:\n",
    "            # Get random action\n",
    "            blue_action = np.random.randint(0, env.ACTION_SPACE_SIZE)\n",
    "\n",
    "        new_state, rewards, done = env.step([red_action, blue_action])\n",
    "\n",
    "        episode_reward += rewards[0]\n",
    "        if episode % SHOW_EVERY == 0 or episode == 1:\n",
    "            env.render()\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        # Every step we update replay memory and train main network\n",
    "        red_agent.update_replay_memory((current_state, red_action, rewards[0], new_state, done))\n",
    "        red_agent.train(done, step)\n",
    "        \n",
    "        blue_agent.update_replay_memory((current_state, blue_action, rewards[1], new_state, done))\n",
    "        blue_agent.train(done, step)\n",
    "\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)\n",
    "\n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "    ep_rewards.append(episode_reward)\n",
    "    best_marker = ''\n",
    "    if epsilon < .25:\n",
    "        if ep_rewards[-1] < best_ep_rewards: \n",
    "            countdown += 1\n",
    "        else:\n",
    "            best_ep_rewards = max(ep_rewards[-1], best_ep_rewards)\n",
    "            countdown = 0\n",
    "            best_marker = '***'\n",
    "\n",
    "    #if countdown >= TERMINATE_COUNT:\n",
    "    #    print(f'No improvement in {countdown} episode.  Terminating training')\n",
    "    #    break\n",
    "    spread = env.state.sum()\n",
    "    if spread > 0:\n",
    "        red_win += 1\n",
    "    elif spread < 0:\n",
    "        blue_win += 1\n",
    "    else:\n",
    "        draw += 1\n",
    "    print(f\"Episode: {episode}, Control: {spread}, Red: {red_win}, Blue: {blue_win}, Draw: {draw}\")#Steps: {step}, Epsilon: {epsilon}, Countdown: {countdown}{best_marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save a trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_agent.model.save_weights(f'models/{MODEL_NAME}_{episode_reward:_>7.2f}_{int(time.time())}.h5') # save our red agent weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seeing a trained agent in action**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"navy_rl_trained.mov\" controls  width=\"600\"  height=\"600\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"navy_rl_trained.mov\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "    <a href=\"#top\">back to top</a>\n",
    "</div>\n",
    "\n",
    "<span style=\"font-size:28px;\">Beyond this Tutorial</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this tutorial is designed to help you see the potential of reinforcement learning for Navy and DoD applications, for a deeper introduction to reinforcement learning please look to the following resoures:\n",
    "\n",
    "* <a href=\"http://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf\">Reinforcement Learning (2nd ed.) by Richard S. Sutton and Andrew G. Barto</a>\n",
    "* <a href=\"https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\">OpenAI's Spinning Up Introduction</a>\n",
    "* <a href=\"https://www.youtube.com/watch?v=yMk_XtIEzH8&list=PLQVvvaa0QuDezJFIOU5wDdfy4e9vdnx-7\">Sentdex's YouTube series</a>\n",
    "* <a href=\"https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ\">David Silver's YouTube series</a>\n",
    "* <a href=\"https://www.freecodecamp.org/news/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe/\">freeCodeCamp</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:28px;\">Credit</span>\n",
    "\n",
    "NAML for allowing us to present this tutorial.\n",
    "\n",
    "Sentdex (YouTube channel linked above) for significant portions of code and motivation for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to view this notebook online:\n",
    "\n",
    "<span style=\"font-size:28px;\">https://bit.ly/2PywzWy</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naml",
   "language": "python",
   "name": "naml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
